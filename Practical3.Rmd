---
title: "Kaminda - Practical 3"
output: html_notebook
---

```{r}
install.packages("gridExtra")
install.packages("jpeg")
install.packages("imager")
install.packages("magick")

BiocManager::install("EBImage")
install.packages("abind")

install.packages("torch")
install.packages("torchvision")
install.packages("luz")
```

```{r}
library(ggplot2)
library(gridExtra)
library(imager)
```

```{r}
library(jpeg)
library(magick)
```

```{r}
library(EBImage)
```

```{r}
library(grid)
```

```{r}
library(dplyr)
```

```{r}
library(abind)
```


###Exploring the dataset

**0**
# Pneumonia is an infection causing inflammation of the alveoli.
# The benefit of being able to identify it from X-rays is that it provides quick diagnosis, in a safe and non-invasive manner.

```{r}
data_folder = "Data/lab3_chest_xray"
files <-list.files(data_folder, full.names = TRUE, recursive = TRUE )
sort(sample(files, 20))
```
**1**
# These file names tell us that pneumonia can be caused by bacteria or virus.
# This might make predicting pneumonia more difficult because X-rays do not detect microorganisms such as bacteria or viruses. Predicting may not be accurate to produce a result that lets us know the cause of pneumonia.

```{r exploring data}
base_dir <- "Data/lab3_chest_xray"

train_pneumonia_dir <- file.path(base_dir, "train", "PNEUMONIA")
train_normal_dir <- file.path(base_dir, "train", "NORMAL")

test_pneumonia_dir <- file.path(base_dir, "test", "PNEUMONIA")
test_normal_dir <- file.path(base_dir, "test", "NORMAL")

val_normal_dir <- file.path(base_dir, "validate", "NORMAL")
val_pneumonia_dir <- file.path(base_dir, "validate", "PNEUMONIA")

train_pn <- list.files(train_pneumonia_dir, full.names = TRUE)
train_normal <- list.files(train_normal_dir, full.names = TRUE)

test_normal <- list.files(test_normal_dir, full.names = TRUE)
test_pn <- list.files(test_pneumonia_dir, full.names = TRUE)

val_pn <- list.files(val_pneumonia_dir, full.names = TRUE)
val_normal <- list.files(val_normal_dir, full.names = TRUE)

cat("Total images:", length(c(train_pn, train_normal, test_normal, test_pn, val_pn, val_normal)), "\n")
```

```{r}
cat("Total pneumonia images:", length(c(train_pn, test_pn, val_pn)), "\n")
```

```{r}
cat("Total Normal images:", length(c(train_normal, test_normal, val_normal)), "\n")
```

###Creating training datasets

```{r training datasets}
train_dataset <- c(train_pn, train_normal)
train_labels <- c(rep("pneumonia", length(train_pn)), rep("normal", length(train_normal)))

test_dataset <- c(test_pn, test_normal)
test_labels <- c(rep("pneumonia", length(test_pn)), rep("normal", length(test_normal)))

val_dataset <- c(val_pn, val_normal)
val_labels <- c(rep("pneumonia", length(val_pn)), rep("normal", length(val_normal)))

#Creating data frames
train_data <- data.frame(dataset = train_dataset, label = train_labels)
test_data <- data.frame(dataset = test_dataset, label = test_labels)
val_data <- data.frame(dataset = val_dataset, label = val_labels)

#Shuffling the data frame
train_data <- train_data[sample(nrow(train_data)), ]
test_data <- test_data[sample(nrow(test_data)), ]
val_data <- val_data[sample(nrow(val_data)), ]

#Extracting shuffled dataset and labels
shuffled_train_dataset <- train_data$dataset
shuffled_train_labels <- train_data$label

shuffled_test_dataset <- test_data$dataset
shuffled_test_labels <- test_data$label

shuffled_val_dataset <- val_data$dataset
shuffled_val_labels <- val_data$label
```

```{r}
cat("file name: ", shuffled_train_dataset[5], "\nlabel: ", shuffled_train_labels[5])
```

###Data visualization

```{r data visualization}

#  List to store the ggplot objects
plots <- list()

# Iterating through the images and labels
for (i in 1:4) {
  
image <- readImage(shuffled_train_dataset[i])
  
# Creating a ggplot object for the image with the corresponding label
  plot <- ggplot() +
    theme_void() +
    annotation_custom(
      rasterGrob(image, interpolate = TRUE),
      xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf
    )
  
# Adding ggplot object to the list
  plots[[i]] <- plot
}

# Arranging plots in a 2x2 grid
grid.arrange(grobs = plots, nrow = 2, ncol = 2)

```
**2**
# From looking at these images, we need to normalize the image size (i.e. dimensions) before training a model. We may also need to normalize the alignment of images.

###Data pre-processing

```{r}
process_images <- function(shuffled_dataset) {
  
img_size <- 224  # Desired image size
  
# Initializing an empty list to store processed images
X <- list()
  
# Looping through each image path in shuffled_train_dataset
for (image_path in shuffled_dataset) {

# Reading the image
img <- imager::load.image(image_path)
    
# Normalizing the image
img_normalized <- img/255
    
# Resizing the image
img_resized <- resize(img_normalized, img_size, img_size)
    
# Appending the processed image to the list
    X <- c(X, list(img_resized))
  }
  
  return(X)
}
```

```{r training, testing, validating}
train_X <- process_images(shuffled_train_dataset)
test_X <- process_images(shuffled_test_dataset)
val_X <- process_images(shuffled_val_dataset)

train_y <- ifelse(shuffled_train_labels == "normal", 1, 2)
test_y <- ifelse(shuffled_test_labels == "normal", 1, 2)
val_y <- ifelse(shuffled_val_labels == "normal", 1, 2)

train_y <- as.integer(train_y)
test_y <- as.integer(test_y)
val_y <- as.integer(val_y)
```

```{r}
# Creating list to store the ggplot objects
plots <- list()

# Iterating through the images and labels
for (i in 1:4) {
  if (train_y[i] == 0) {
    label <- "Normal"
  } else {
    label <- "Pneumonia"
  }
  
# Creating a ggplot object for the image with the corresponding label
  plot <- ggplot() +
    theme_void() +
    ggtitle(label) +
    annotation_custom(
      rasterGrob(train_X[[i]], interpolate = TRUE),
      xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf
    )
  
# Adding the ggplot object to the list
  plots[[i]] <- plot
}

# Arranging the plots in a 2x2 grid
grid.arrange(grobs = plots, nrow = 2, ncol = 2)
```

```{r}
# Combining train, test, and val vectors into a single data frame
df <- data.frame(
  Data = rep(c("Train", "Test", "Val"), times = c(length(train_y), length(test_y), length(val_y))),
  Value = c(train_y, test_y, val_y)
)

# Creating a single bar plot with facets
fig <- ggplot(df, aes(x = Value)) +
  geom_bar() +
  ylim(0, 510) +
  facet_wrap(~Data, ncol = 3)

# Arranging the plot
grid.arrange(fig, nrow = 1)
```

**3**
# If the dataset was not balanced


###Training

```{r reshaping dataset}
train_X <- array(data = unlist(train_X), dim = c(1000, 224, 224, 1))
test_X <- array(data = unlist(test_X), dim = c(200, 224, 224, 1))
val_X <- array(data = unlist(val_X), dim = c(16, 224, 224, 1))
```

```{r}
print(dim(train_X))
print(length(train_y))
print(dim(test_X))
print(length(test_y))
print(dim(val_X))
print(length(val_y))
```
```{r aperm}
train_X <- aperm(train_X, c(1,4,2,3))
test_X <- aperm(test_X, c(1,4,2,3))
val_X <- aperm(val_X,c(1,4,2,3))

dim(train_X)
```

```{r}
library(torch)
library(torchvision)
library(luz)
```

```{r custom dataset}

# Defining a custom dataset class
ImageDataset <- dataset(
  name = "ImageDataset",
  initialize = function(X, y) {
    
# Storing the data as tensors
    self$data <- torch_tensor(X)
    self$labels <- torch_tensor(y)
  },
  .getitem = function(i) {
    
# Returning a single sample and label
    x <- self$data[i,,,]
    y <- self$labels[i]
    list(x = x, y = y)
  },
  .length = function() {
    
# Returning the number of samples
    dim(self$data)[1]
  }
)

# Creating a dataset object from data
train_dataset <- ImageDataset(train_X, train_y)
test_dataset <- ImageDataset(test_X, test_y)
val_dataset <- ImageDataset(val_X, val_y)

# Creating a dataloader object from dataset
train_dataloader <- dataloader(train_dataset, batch_size = 16)
test_dataloader <- dataloader(test_dataset, batch_size = 16)
val_dataloader <- dataloader(val_dataset, batch_size = 16)

# Iterating over batches of data
batch = train_dataloader$.iter()$.next()

# Visualizing the first batch size
batch[[1]]$size()
```

###Creating a CNN Model

```{r CNN Model}
net <- nn_module(
  "Net",
  
  initialize = function() {
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 1)
    self$dropout1 <- nn_dropout2d(0.25)
    self$dropout2 <- nn_dropout2d(0.5)
    self$fc1 <- nn_linear(774400, 128)  # Adjusting input size based on your image dimensions
    
    self$fc2 <- nn_linear(128, 2)             # Changing the output size to match classification task
  },
  
  forward = function(x) {
    x %>%                                        # N * 1 * 224 * 224
      self$conv1() %>%                           # N * 32 * 222 * 222
      nnf_relu() %>% 
      self$conv2() %>%                           # N * 64 * 220 * 220
      nnf_relu() %>% 
      nnf_max_pool2d(2) %>%                      # N * 64 * 110 * 110
      self$dropout1() %>% 
      torch_flatten(start_dim = 2) %>%           # N * 64 * 110 * 110 --> N * 774400
      self$fc1() %>%                             # N * 128
      nnf_relu() %>% 
      self$dropout2() %>% 
      self$fc2()                                 # N * 2 (change the output size to match your classification task)
  }
)
```

##Training Model

```{r}
library(abind)
library(EBImage)
library(imager)
```

```{r}
library(torch)
library(torchvision)
library(luz)
```

```{r warning=FALSE}
# Setting the number of epochs

num_epochs <- 3

train_loss <- numeric(num_epochs)
train_acc <- numeric(num_epochs)
test_loss <- numeric(num_epochs)
test_acc <- numeric(num_epochs)

# Looping through the epochs
for (epoch in 0:num_epochs) {
  
# Performing training and validation for each epoch
  fitted <- net %>%
    setup(
      loss = nn_cross_entropy_loss(),
      optimizer = optim_adam,
      metrics = list(
        luz_metric_accuracy()
      )
    ) %>%
    fit(train_dataloader, epochs = 1, valid_data = test_dataloader)
  
# Printing the metrics for the current epoch
  cat("Epoch ", epoch, "/", num_epochs, "\n")
  cat("Train metrics: Loss: ", fitted$records$metrics$train[[1]]$loss, " - Acc: ", fitted$records$metrics$train[[1]]$acc, "\n")
  cat("Valid metrics: Loss: ", fitted$records$metrics$valid[[1]]$loss, " - Acc: ", fitted$records$metrics$valid[[1]]$acc, "\n")
  cat("\n")
  
# Storing the loss and accuracy values
  train_loss[epoch] <- fitted$records$metrics$train[[1]]$loss
  train_acc[epoch] <- fitted$records$metrics$train[[1]]$acc
  test_loss[epoch] <- fitted$records$metrics$train[[1]]$loss
  test_acc[epoch] <- fitted$records$metrics$valid[[1]]$acc
}
```

